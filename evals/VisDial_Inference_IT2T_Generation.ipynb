{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bc991",
   "metadata": {},
   "source": [
    "# FROMAGe Visual Dialog (Text Generation)\n",
    "\n",
    "This is a notebook showcasing the VisDial image-and-text-to-text (IT2T) results from our paper, [Grounding Language Models to Images for Multimodal Generation](https://arxiv.org/abs/2301.13823). This result is reported in Table 2 of the paper. This is the standard [VisDial](https://arxiv.org/abs/1611.08669) evaluation, which measures the ability of models to pick out the correct text answer out of 100 options.\n",
    "\n",
    "At least 18GB of GPU memory is required to run FROMAGe, and it has only been tested on A6000, V100, and 3090 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import logging\n",
    "from tqdm import notebook\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fromage import models\n",
    "from fromage import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e884127",
   "metadata": {},
   "source": [
    "### Load Pretrained FROMAGe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4646a124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n",
      "Using facebook/opt-6.7b for the language model.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 1 visual tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf51dce17a5479992af55c8dbf341bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the LM.\n",
      "Initializing embedding for the retrieval token [RET] (id = 50266).\n",
      "Restoring pretrained weights for the visual model.\n",
      "Freezing the VM.\n"
     ]
    }
   ],
   "source": [
    "# Load model used in the paper.\n",
    "model_dir = './fromage_model/'\n",
    "model = models.load_fromage(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d373b",
   "metadata": {},
   "source": [
    "### VisDial\n",
    "\n",
    "Download the VisDial validation [annotations](https://www.dropbox.com/s/ibs3a0zhw74zisc/visdial_1.0_val.zip?dl=0), the [dense answer annotations](https://www.dropbox.com/s/3knyk09ko4xekmc/visdial_1.0_val_dense_annotations.json?dl=0) (for computing MRR) and the [images](https://www.dropbox.com/s/twmtutniktom7tu/VisualDialog_val2018.zip?dl=0). Extract everything to the `VisualDialog` folder.\n",
    "\n",
    "First, we'll load the annotations, and define the paths to our images and annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf39013",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'VisualDialog/'\n",
    "split = 'val'\n",
    "img_dir = os.path.join(base_dir, f'VisualDialog_{split}2018')\n",
    "\n",
    "with open(os.path.join(base_dir, f'visdial_1.0_{split}.json'), 'r') as f:\n",
    "    visdial_data = json.load(f)\n",
    "    \n",
    "with open(os.path.join(base_dir, f'visdial_1.0_{split}_dense_annotations.json'), 'r') as f:\n",
    "    dense_data = json.load(f)\n",
    "\n",
    "# Check that dense and sparse data are aligned.\n",
    "assert len(dense_data) == len(visdial_data['data']['dialogs'])\n",
    "for i in range(len(dense_data)):\n",
    "    assert dense_data[i]['image_id'] == visdial_data['data']['dialogs'][i]['image_id']\n",
    "    \n",
    "questions = visdial_data['data']['questions']\n",
    "answers = visdial_data['data']['answers']\n",
    "dialogs = visdial_data['data']['dialogs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4ae3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_values_from_path(path: str, feature_extractor):\n",
    "    \"\"\"Helper function for getting images pixels from a local path.\"\"\"\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    pixel_values = utils.get_pixel_values_for_model(feature_extractor, img)\n",
    "    if torch.cuda.is_available():\n",
    "        pixel_values = pixel_values.bfloat16()\n",
    "        pixel_values = pixel_values.cuda()\n",
    "    return pixel_values[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944691a6",
   "metadata": {},
   "source": [
    "Then, for each VisDial example, we compute the loss conditioned on the image and the preceding dialogue. We return the option with the lowest loss as the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20c3c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4c377d84004a969c6603604dd62540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topk = (1, 5, 10)\n",
    "# Number of options in a batch to compute loss for.\n",
    "# If using a GPU with lower VRAM, this may have to be lowered.\n",
    "batch_size = 20\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='none').cuda()\n",
    "\n",
    "# Save intermediate results to a numpy file, to allow resuming in case of interruptions.\n",
    "save_path = 'visdial_results_full.npy'\n",
    "if os.path.exists(save_path):\n",
    "    with open(save_path, 'rb') as rf:\n",
    "        all_data = np.load(rf, allow_pickle=True).item()\n",
    "        all_preds = all_data['all_preds']\n",
    "        all_gt_results = all_data['all_gt_results']\n",
    "        all_losses = all_data['all_losses']\n",
    "        assert len(all_preds) == len(all_gt_results) == len(all_losses)\n",
    "else:\n",
    "    # No in progress data, initialize from scratch.\n",
    "    all_preds = []\n",
    "    all_gt_results = []\n",
    "    all_losses = []\n",
    "\n",
    "for example_idx in notebook.tqdm(range(len(all_preds) // 10, len(dialogs))):\n",
    "    dialog = dialogs[example_idx]\n",
    "    image_id = str(dialog['image_id']).rjust(12, '0')\n",
    "    contexts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = get_pixel_values_from_path(\n",
    "            os.path.join(img_dir, f'VisualDialog_{split}2018_{image_id}.jpg'),\n",
    "            model.model.feature_extractor)\n",
    "        visual_embs = model.model.get_visual_embs(images, mode='captioning')\n",
    "\n",
    "        for i in range(len(dialog['dialog'])):\n",
    "            prev_d = dialog['dialog'][i-1]\n",
    "            current_d = dialog['dialog'][i]\n",
    "            if i > 0:\n",
    "                contexts.append('A: ' + answers[prev_d['answer']])\n",
    "            contexts.append('Q: ' + questions[current_d['question']] + '?')\n",
    "            answer_options = [answers[i] for i in current_d['answer_options']]\n",
    "            answer = answers[current_d['answer']]\n",
    "            gt_index = current_d['gt_index']\n",
    "            caption = '\\n'.join(contexts) + '\\nA: '\n",
    "\n",
    "            # Run through every possible option, and pick the option with the lowest loss (= lowest perplexity)\n",
    "            example_losses = []\n",
    "            # Tokenize the dialogue sequence (as this is the same for all answer choices).\n",
    "            caption_ids = model.model.tokenizer(\n",
    "                caption, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "            caption_ids = caption_ids.to(images.device)\n",
    "            caption_embs = model.model.input_embeddings(caption_ids)  # (N, T, D)\n",
    "            condition_length = visual_embs.shape[1] + caption_embs.shape[1]\n",
    "\n",
    "            all_example_embs = []\n",
    "            all_example_labels = []\n",
    "\n",
    "            for _, ans in enumerate(answer_options):\n",
    "                ans_ids = model.model.tokenizer(ans, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "                ans_ids = ans_ids.to(images.device)\n",
    "                ans_embs = model.model.input_embeddings(ans_ids)\n",
    "                input_embs = torch.cat([\n",
    "                    visual_embs,\n",
    "                    caption_embs,\n",
    "                    ans_embs], dim=1)\n",
    "                labels = torch.cat([\n",
    "                    torch.zeros(visual_embs.shape[:-1], device=caption_ids.device, dtype=caption_ids.dtype) - 100,\n",
    "                    caption_ids,\n",
    "                    ans_ids], dim=1)\n",
    "                assert labels.shape[1] == input_embs.shape[1]\n",
    "\n",
    "                all_example_embs.append(input_embs)\n",
    "                all_example_labels.append(labels)\n",
    "\n",
    "            max_len = max([x.shape[1] for x in all_example_labels])\n",
    "            padded_example_embs = [torch.nn.functional.pad(x, (0, 0, 0, max_len - x.shape[1])) for x in all_example_embs]\n",
    "            padded_example_embs = torch.cat(padded_example_embs, axis=0)\n",
    "\n",
    "            padded_example_labels = [torch.nn.functional.pad(x, (0, max_len - x.shape[1]), value=-100) for x in all_example_labels]\n",
    "            padded_example_labels = torch.cat(padded_example_labels, axis=0)\n",
    "\n",
    "            all_logits = []\n",
    "            batches = int(padded_example_embs.shape[0] // batch_size)\n",
    "            for i in range(batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                out = model.model.lm(\n",
    "                    inputs_embeds=padded_example_embs[start_idx:end_idx, ...],\n",
    "                    labels=None,\n",
    "                    use_cache=False,\n",
    "                    output_hidden_states=True)\n",
    "                all_logits.append(out.logits)\n",
    "\n",
    "            logits = torch.cat(all_logits, dim=0)\n",
    "            example_losses = ce_loss(logits.reshape((-1, logits.shape[-1])), padded_example_labels.reshape((-1,)))\n",
    "            example_losses = example_losses.reshape((100, max_len))[:, condition_length:]\n",
    "            example_losses = example_losses.sum(axis=1)\n",
    "\n",
    "            all_losses.append(example_losses.cpu().float().numpy())\n",
    "            scores = -example_losses\n",
    "            _, preds = scores.topk(max(topk))\n",
    "            all_preds.append(preds)\n",
    "            all_gt_results.append(gt_index)\n",
    "\n",
    "    with open(save_path, 'wb') as wf:\n",
    "        np.save(wf, {'all_preds': all_preds, 'all_gt_results': all_gt_results, 'all_losses': all_losses})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2a81a",
   "metadata": {},
   "source": [
    "### Computing Results\n",
    "\n",
    "Finally, we can compute NDCG, MRR, and Recall@k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0b673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some classes to help us compute NDCG and MRR.\n",
    "# Modified from https://github.com/batra-mlp-lab/visdial-challenge-starter-pytorch/blob/master/visdialch/metrics.py\n",
    "\n",
    "class NDCG(object):\n",
    "    def __init__(self):\n",
    "        self._ndcg_numerator = 0.0\n",
    "        self._ndcg_denominator = 0.0\n",
    "\n",
    "    def observe(\n",
    "            self, predicted_scores: torch.Tensor, target_relevance: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Observe model output scores and target ground truth relevance and\n",
    "        accumulate NDCG metric.\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted_scores: torch.Tensor\n",
    "            A tensor of shape (batch_size, num_options), because dense\n",
    "            annotations are available for 1 randomly picked round out of 10.\n",
    "        target_relevance: torch.Tensor\n",
    "            A tensor of shape same as predicted scores, indicating ground truth\n",
    "            relevance of each answer option for a particular round.\n",
    "        \"\"\"\n",
    "        predicted_scores = predicted_scores.detach()\n",
    "\n",
    "        # shape: (batch_size, 1, num_options)\n",
    "        predicted_scores = predicted_scores.unsqueeze(1)\n",
    "        predicted_ranks = scores_to_ranks(predicted_scores)\n",
    "\n",
    "        # shape: (batch_size, num_options)\n",
    "        predicted_ranks = predicted_ranks.squeeze(1)\n",
    "        batch_size, num_options = predicted_ranks.size()\n",
    "\n",
    "        k = torch.sum(target_relevance != 0, dim=-1)\n",
    "\n",
    "        # shape: (batch_size, num_options)\n",
    "        _, rankings = torch.sort(predicted_ranks, dim=-1)\n",
    "        # Sort relevance in descending order so highest relevance gets top rnk.\n",
    "        _, best_rankings = torch.sort(\n",
    "            target_relevance, dim=-1, descending=True\n",
    "        )\n",
    "\n",
    "        # shape: (batch_size, )\n",
    "        batch_ndcg = []\n",
    "        for batch_index in range(batch_size):\n",
    "            num_relevant = k[batch_index]\n",
    "            dcg = self._dcg(\n",
    "                rankings[batch_index][:num_relevant],\n",
    "                target_relevance[batch_index],\n",
    "            )\n",
    "            best_dcg = self._dcg(\n",
    "                best_rankings[batch_index][:num_relevant],\n",
    "                target_relevance[batch_index],\n",
    "            )\n",
    "            batch_ndcg.append(dcg / best_dcg)\n",
    "\n",
    "        self._ndcg_denominator += batch_size\n",
    "        self._ndcg_numerator += sum(batch_ndcg)\n",
    "\n",
    "    def _dcg(self, rankings: torch.Tensor, relevance: torch.Tensor):\n",
    "        sorted_relevance = relevance[rankings].cpu().float()\n",
    "        discounts = torch.log2(torch.arange(len(rankings)).float() + 2)\n",
    "        return torch.sum(sorted_relevance / discounts, dim=-1)\n",
    "\n",
    "    def retrieve(self, reset: bool = True, key=\"\"):\n",
    "        if self._ndcg_denominator > 0:\n",
    "            metrics = {\n",
    "                key + \"ndcg\": float(self._ndcg_numerator / self._ndcg_denominator)\n",
    "            }\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self._ndcg_numerator = 0.0\n",
    "        self._ndcg_denominator = 0.0\n",
    "        \n",
    "\n",
    "def scores_to_ranks(scores: torch.Tensor):\n",
    "    \"\"\"Convert model output scores into ranks.\"\"\"\n",
    "    batch_size, num_rounds, num_options = scores.size()\n",
    "    scores = scores.view(-1, num_options)\n",
    "\n",
    "    # sort in descending order - largest score gets highest rank\n",
    "    sorted_ranks, ranked_idx = scores.sort(1, descending=True)\n",
    "\n",
    "    # i-th position in ranked_idx specifies which score shall take this\n",
    "    # position but we want i-th position to have rank of score at that\n",
    "    # position, do this conversion\n",
    "    ranks = ranked_idx.clone().fill_(0)\n",
    "    for i in range(ranked_idx.size(0)):\n",
    "        for j in range(num_options):\n",
    "            ranks[i][ranked_idx[i][j]] = j\n",
    "    # convert from 0-99 ranks to 1-100 ranks\n",
    "    ranks += 1\n",
    "    ranks = ranks.view(batch_size, num_rounds, num_options)\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e4c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'rb') as rf:\n",
    "    all_data = np.load(rf, allow_pickle=True).item()\n",
    "    all_preds = all_data['all_preds']\n",
    "    all_gt_results = all_data['all_gt_results']\n",
    "    all_losses = all_data['all_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7686317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-k, k=1, acc=0.17573\n",
      "top-k, k=5, acc=0.19971\n",
      "top-k, k=10, acc=0.24414\n",
      "top-k, k=20, acc=0.48309\n",
      "MRR: 0.21997\n",
      "NDCG: 0.16594\n"
     ]
    }
   ],
   "source": [
    "top_k_accuracy = collections.defaultdict(list)\n",
    "mrr_results = []\n",
    "all_ranks = []\n",
    "topk = (1, 5, 10, 20)\n",
    "ndcg = NDCG()\n",
    "\n",
    "assert len(all_preds) == len(all_gt_results)\n",
    "for gt, loss in zip(all_gt_results, all_losses):\n",
    "    scores = -loss\n",
    "    _, preds = torch.tensor(scores).topk(100)\n",
    "    rank = np.where(preds == gt)[0][0] + 1\n",
    "    all_ranks.append(rank)\n",
    "    mrr_results.append(1 / rank)\n",
    "\n",
    "    for k in topk:\n",
    "        acc = gt in preds[:k]\n",
    "        top_k_accuracy[k].append(acc)\n",
    "        \n",
    "dense_mrr = []\n",
    "for i in range(len(dense_data)):\n",
    "    idx = i * 10 + dense_data[i]['round_id']\n",
    "    if idx >= len(all_losses):\n",
    "        break\n",
    "    scores = -torch.tensor(all_losses[idx])[None, :]\n",
    "    relevance = torch.tensor(dense_data[i]['gt_relevance'])[None, :]\n",
    "    ndcg.observe(scores, relevance)\n",
    "    dense_mrr.append(mrr_results[idx])\n",
    "\n",
    "for k in topk:\n",
    "    print(f'top-k, k={k}, acc={np.mean(top_k_accuracy[k]):.5f}')\n",
    "print(f'MRR: {np.mean(mrr_results):.5f}')\n",
    "print(f'NDCG: {ndcg.retrieve(reset=True)[\"ndcg\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
